{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80bb286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from gensim.models import Word2Vec\n",
    "# import multiprocessing\n",
    "\n",
    "# df = pd.read_csv(\"text-generator.csv\",  encoding='cp1252')\n",
    "\n",
    "# def clean_data(text):\n",
    "#     text = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿ/]+', '', text)\n",
    "#     text = re.sub(r'[\\\\/×\\^\\]\\[÷]', '', text)\n",
    "#     return text\n",
    "# def change_lower(text):\n",
    "#     text = text.lower()\n",
    "#     return text\n",
    "\n",
    "# stopwords_list = stopwords.words(\"english\")\n",
    "# def remover(text):\n",
    "#     text_tokens = text.split(\" \")\n",
    "#     final_list = [word for word in text_tokens if not word in stopwords_list]\n",
    "#     text = ' '.join(final_list)\n",
    "# #     print(text)\n",
    "#     return text\n",
    "\n",
    "# def get_w2vdf(df):\n",
    "#     w2v_df = pd.DataFrame(df[\"Text\"]).values.tolist()\n",
    "#     for i in range(len(w2v_df)):\n",
    "#         w2v_df[i] = w2v_df[i][0].split(\" \")\n",
    "#     return w2v_df\n",
    "\n",
    "# def train_w2v(w2v_df):\n",
    "#     cores = multiprocessing.cpu_count()\n",
    "#     w2v_model = Word2Vec(min_count=4,\n",
    "#                          window=4,\n",
    "# #                          size=300, \n",
    "#                          alpha=0.03, \n",
    "#                          min_alpha=0.0007, \n",
    "#                          sg = 1,\n",
    "#                          workers=cores-1)\n",
    "    \n",
    "#     w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "#     w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "#     return w2v_model\n",
    "\n",
    "# df[[\"Text\"]] = df[[\"Text\"]].astype(str)\n",
    "# df[\"Text\"] = df[\"Text\"].apply(change_lower)\n",
    "# df[\"Text\"] = df[\"Text\"].apply(clean_data)\n",
    "# df[\"Text\"] = df[\"Text\"].apply(remover)\n",
    "\n",
    "# w2v_df = get_w2vdf(df)\n",
    "# w2v_model = train_w2v(w2v_df)\n",
    "# # print(w2v_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08460fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review in df['Text']:\n",
    "  sents=tokenizer.tokenize(review.strip())\n",
    "  sum+=len(sents)\n",
    "  for sent in sents:\n",
    "#     cleaned_sent=clean_reviews(sent)\n",
    "    sentences.append(sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))  # total no of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2dbd7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12119494,  0.34125713,  0.20713627,  0.1709707 , -0.18127331,\n",
       "        0.03996315,  0.29467914,  0.28692764, -0.36901525, -0.00207038,\n",
       "        0.358314  ,  0.16404672, -0.17583953,  0.06934   ,  0.05710686,\n",
       "       -0.02452595, -0.11645096, -0.01343467, -0.12369075, -0.37669498,\n",
       "        0.20750453, -0.09735931,  0.02207841,  0.01602149, -0.01123085,\n",
       "        0.01394659, -0.07664207, -0.06457649,  0.02797045,  0.30245718,\n",
       "       -0.17128928,  0.03568438, -0.01372833,  0.3231812 , -0.33298925,\n",
       "       -0.15583357, -0.01084761,  0.05087924,  0.04332731, -0.14096728,\n",
       "       -0.00066474, -0.09403183, -0.03481948,  0.08074769,  0.39247745,\n",
       "       -0.0908554 , -0.27030477,  0.3244035 ,  0.34192628,  0.01945674,\n",
       "        0.08710952, -0.21660838, -0.05753756, -0.2152091 , -0.25447187,\n",
       "       -0.11703781,  0.17809491,  0.0218898 ,  0.22927998, -0.15148245,\n",
       "       -0.03188246, -0.06462417, -0.05065012,  0.10870004, -0.39706904,\n",
       "        0.13400418,  0.03219141,  0.20778579, -0.21537575,  0.13175103,\n",
       "        0.18802753,  0.10304314, -0.02201849, -0.39127672,  0.06644648,\n",
       "       -0.08605768,  0.23347445,  0.03813516,  0.11989236,  0.02509476,\n",
       "       -0.06244277,  0.0179851 , -0.16110906,  0.32257417, -0.3544533 ,\n",
       "       -0.1507397 ,  0.20027512,  0.23306987, -0.31312767,  0.0672806 ,\n",
       "        0.19186647,  0.3010857 ,  0.07173882, -0.05986751,  0.15467933,\n",
       "        0.21434818, -0.12758155, -0.15985584,  0.02305048, -0.1622212 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector('information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c67907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('government', 0.4234906733036041),\n",
       " ('transactions', 0.4179198741912842),\n",
       " ('family', 0.3950233459472656),\n",
       " ('needs', 0.38710522651672363),\n",
       " ('numbers', 0.3843950927257538),\n",
       " ('support', 0.37131965160369873),\n",
       " ('disclose', 0.3637078106403351),\n",
       " ('authorities', 0.359550803899765),\n",
       " ('behalf', 0.35748448967933655),\n",
       " ('merchants', 0.35155895352363586)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d59c430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3079453"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity('information','data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9882488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of words : 657\n"
     ]
    }
   ],
   "source": [
    "print(\"The no of words :\",len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab432f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83d99ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of key-value pairs :  657\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict={}\n",
    "for word in w2v_model.wv.index_to_key:\n",
    "    word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98fb19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "maxi=-1\n",
    "for i,rev in enumerate(df['Text']):\n",
    "    tokens=rev.split()\n",
    "    if(len(tokens)>maxi):\n",
    "        maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55e6b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['Text'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deee1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rev_len=147  # max lenght of a texte\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=100 # embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "138e4d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 147)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e710ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "    embed_vector=word_vec_dict.get(word)\n",
    "    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "        embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e50dcdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03509673  0.65169078 -0.48518682  0.43348512  0.47571629 -0.14120139\n",
      " -0.5007906  -0.08653297 -0.48139405 -0.34685338 -0.1391917   0.2740941\n",
      " -0.84675139 -0.32729667  0.57380289  0.5494774   0.13669755 -0.20296624\n",
      " -0.67501092 -0.67814964  0.37982097 -0.27690583  0.36609346 -0.31082335\n",
      "  0.26088044  0.07762034 -0.13885926 -0.1397426  -0.48620063  0.73688012\n",
      "  0.25937539  0.55652648  0.01769175  0.13391206 -0.01005688  0.05293911\n",
      " -0.24077915  0.8105945   0.19337034 -0.12585549 -0.33557549  0.2433756\n",
      "  0.03117934 -0.11962032  0.51375318 -0.03263777  0.23306128 -0.08587485\n",
      "  0.87020373 -0.21787582 -0.00667506 -0.3920128  -0.28369379 -0.13557704\n",
      "  0.1839419   0.47875446  0.62673849  0.31227276 -0.39770517  0.20264953\n",
      "  0.03453618  0.50732619 -0.10847239  0.08083101 -0.14297186 -0.3182106\n",
      "  0.09078025 -0.05982817 -0.23254313  0.29192856  0.0241233  -0.04972333\n",
      " -0.29940939 -0.12638225  0.3599107  -0.21960202 -0.00275448  0.3285495\n",
      "  0.37006816  0.0878973   0.00571305  0.24005665 -0.35369515  0.65678996\n",
      "  0.01739097  0.3879064   0.34488711  0.18108     0.04196634 -0.2139591\n",
      "  0.30380782  0.47082418  0.15487081  0.26561075  0.0010676  -0.58503747\n",
      "  0.06132448 -0.18467258 -0.13204873  0.1464465 ]\n"
     ]
    }
   ],
   "source": [
    "print(embed_matrix[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0d8a254",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m in_words\u001b[38;5;241m=\u001b[39m in_txt\n\u001b[0;32m      7\u001b[0m input_texts\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(in_words)]\n\u001b[1;32m----> 8\u001b[0m input_words\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m in_words\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Add 'start' at start and 'end' at end of text\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tr_words\u001b[38;5;241m=\u001b[39m clean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtr_txt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_words' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "\n",
    "# Pass the input records and taret records\n",
    "for in_txt,tr_txt in zip(input_data,target_data):\n",
    "    in_words= in_txt\n",
    "    input_texts+= [' '.join(in_words)]\n",
    "    input_words+= in_words\n",
    "    # Add 'start' at start and 'end' at end of text\n",
    "    tr_words= clean(\"start \"+tr_txt+\" end\")\n",
    "    target_texts+= [' '.join(tr_words)]\n",
    "    target_words+= tr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16b74cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [365, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m target_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# target_data=keras.utils.to_categorical(df['Summary'])  # one hot target as required by NN.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m x_train,x_test,y_train,y_test\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_rev\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m x_train\n",
      "File \u001b[1;32md:\\esi\\pfe\\transpvisback\\transpvisback\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32md:\\esi\\pfe\\transpvisback\\transpvisback\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\esi\\pfe\\transpvisback\\transpvisback\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [365, 0]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "# target_data=keras.utils.to_categorical(df['Summary'])  # one hot target as required by NN.\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,target_texts,test_size=0.1,random_state=40)\n",
    "x_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
